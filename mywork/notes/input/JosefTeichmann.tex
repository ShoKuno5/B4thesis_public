\section{Lectures}

\begin{enumerate}
    \item Reservoir Computing\begin{enumerate}
        \item \href{https://www.youtube.com/watch?v=AhxROWo_FOw}{Reservoir Computing for SDEs(Josef Teichmann:)}
        \item \href{https://www.youtube.com/watch?v=lak3OjvE_44}{Reservoir Computing \& Dynamical Systems - Second Sumposium on Machine Learning and Dynamical Systems(Josef Teichmann)}
        \item \href{https://www.youtube.com/watch?v=wbH4En-k5Gs}{Introduction to Next Generation Reservoir Computing(Daniel Gauthier)}
    \end{enumerate}
    \item Machine Learning in general\begin{enumerate}
        \item \href{https://people.math.ethz.ch/~jteichma/index.php?content=teach_mlf2023}{Machine Learning in Finance(Josef Teichmann)}
    \end{enumerate}
\end{enumerate}

\subsection{Josef Teichmann: Reservoir Computing for SDEs}
\href{https://www.youtube.com/watch?v=AhxROWo_FOw}{Access from here.}
\begin{enumerate}
    \item We consider differential equations of the form
    $$
    d Y_t=\sum_i V_i\left(Y_t\right) d u_t^i, Y_0=y \in E
    $$
    to construction evolutions in state space $E$ (could be a manifold of finite or infinite dimension) depending on local characteristics, initial value $y \in E$ and the control $u$.
    \item Theorem (Universality)
    Let Evol be a smooth evolution operator on a convenient vector space $E$ which satisfies (again the time derivative is taken with respect to the forward variable $t$ ) a controlled ordinary differential equation
    $$
    d \mathrm{Evol}_{s, t}(x)=\sum_{i=1}^d V_i\left(\mathrm{Evol}_{s, t}(x)\right) d u^i(t) .
    $$
    Then for any smooth (test) function $f: E \rightarrow \mathbb{R}$ and for every $M \geq 0$ there is a time-homogenous linear $W=W\left(V_1, \ldots, V_d, f, M, x\right)$ from $\mathbb{A}_d^M$ to the real numbers $\mathbb{R}$ such that
    $$
    f\left(\operatorname{Evol}_{s, t}(x)\right)=W\left(\pi_M\left(\operatorname{Sig}_{s, t}(1)\right)\right)+\mathcal{O}\left((t-s)^{M+1}\right)
    $$
    for $s \leq t$
    \item Signature as universal dynamical system \begin{enumerate}
        \item This explains that any solution can be represented - up to a linear readout - by a universal reservoir, namely signature. Similar constructions can be done in regularity structures, too (branched rough paths, etc).
        \item This is used in many instances of provable machine learning by, e.g., groups in Oxford (Harald Oberhauser, Terry Lyons, etc), and also ...
        \item ... at JP Morgan, in particular great recent work on 'Nonparametric pricing and hedging of exotic derivatives' by Terry Lyons, Sina Nejad and Imanol Perez Arribas.
        \item in contrast to reservoir computing: signature is high dimensional (i.e. infinite dimensional) and a precisely defined, non-random object.
        \item Can we approximate signature by a lower dimensional random object with similar properties?
    \end{enumerate}
\end{enumerate}

\clearpage